# Scaling and Normalization in Machine Learning

## Introduction

Scaling and normalization are essential preprocessing techniques in machine learning that transform data into a more suitable form for model training. These techniques help in:

- Speeding up convergence during model training
- Improving model performance
- Handling features with different units and scales
- Preventing some features from dominating others due to their magnitude

## Why Scale or Normalize?

Many machine learning algorithms perform better when features are on similar scales. Consider this simple dataset:

| House ID | Size (sq ft) | Age (years) | Price ($) |
|----------|--------------|-------------|-----------|
| 1        | 2,500        | 15          | 350,000   |
| 2        | 1,800        | 7           | 280,000   |
| 3        | 3,200        | 25          | 420,000   |

Without scaling:
- Size ranges from 1,800 to 3,200
- Age ranges from 7 to 25
- Price ranges from 280,000 to 420,000

These vastly different scales can cause algorithms to:
- Give too much importance to features with larger values
- Converge slowly during optimization
- Produce unstable results

## Common Scaling and Normalization Techniques

| Technique | Formula | Range | When to Use |
|-----------|---------|-------|-------------|
| Min-Max Scaling | (x - min(x)) / (max(x) - min(x)) | [0, 1] | When you need bounded values |
| Standardization | (x - mean(x)) / std(x) | Typically [-3, 3] | When you need zero-mean, unit-variance |
| Robust Scaling | (x - median(x)) / IQR(x) | Varies | When data has outliers |
| Log Transform | log(x) | Varies | For highly skewed data |
| Unit Vector Transformation | x / ||x|| | [-1, 1] | For text data, cosine similarity |

## Choosing the Right Technique

- **Min-Max Scaling**: Use for image data, neural networks, or when you need values in a specific range
- **Standardization**: Use for PCA, SVM, linear/logistic regression, and clustering algorithms
- **Robust Scaling**: Use when data contains outliers
- **Log Transform**: Use for right-skewed data or when dealing with varying orders of magnitude

## Code Example: Basic Scaling

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

# Sample data
data = pd.DataFrame({
    'Size': [2500, 1800, 3200],
    'Age': [15, 7, 25],
    'Price': [350000, 280000, 420000]
})

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
data_minmax = min_max_scaler.fit_transform(data)
data_minmax = pd.DataFrame(data_minmax, columns=data.columns)

# Standardization
std_scaler = StandardScaler()
data_std = std_scaler.fit_transform(data)
data_std = pd.DataFrame(data_std, columns=data.columns)

print("Original Data:\n", data)
print("\nMin-Max Scaled Data:\n", data_minmax)
print("\nStandardized Data:\n", data_std)
```

## Key Considerations

1. **Apply the same scaling** to both training and test sets
2. **Fit scalers only on training data** to prevent data leakage
3. **Save scaling parameters** for future predictions
4. **Scale target variables carefully** - sometimes it's better to leave them unscaled
5. **Consider the algorithm** - some methods (like tree-based) don't require scaling

## Next Steps

For more detailed information on implementation and advanced techniques, see the companion file: `understanding-Scaling-and-Normalization.md`
